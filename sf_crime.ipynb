{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF Crime Data Analysis with PySpark\n",
    "\n",
    "In this notebook we use PySpark to explore a dataset from the San Francisco Police Department.\n",
    "Dataset link: [https://data.sfgov.org/Public-Safety/sf-data/skgt-fej3/data](https://data.sfgov.org/Public-Safety/sf-data/skgt-fej3/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SF Crime Analysis\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print('SparkSession started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV data using an RDD\n",
    "from csv import reader\n",
    "\n",
    "# Read all lines from the CSV file\n",
    "crime_data_lines = sc.textFile('data/sf_crime.csv')\n",
    "\n",
    "# Convert each line to a list of strings (removing extra quotes)\n",
    "df_crimes = crime_data_lines.map(lambda line: [x.strip('\"') for x in next(reader([line]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract header and filter it out\n",
    "header = df_crimes.first()\n",
    "print(\"Header:\", header)\n",
    "crimes = df_crimes.filter(lambda x: x != header)\n",
    "print(\"First two data rows:\", crimes.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the RDD of lists to an RDD of Rows\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def list_to_row(keys, values):\n",
    "    row_dict = dict(zip(keys, values))\n",
    "    return Row(**row_dict)\n",
    "\n",
    "rdd_rows = crimes.map(lambda x: list_to_row(header, x))\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "print(\"DataFrame created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame (first 20 rows)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace columns 'X' and 'Y' with 'Longitude' and 'Latitude' and cast them as float\n",
    "df = df.withColumn('X', df['X'].cast('float')).withColumn('Y', df['Y'].cast('float'))\n",
    "df = df.withColumnRenamed('X', 'Longitude').withColumnRenamed('Y', 'Latitude')\n",
    "print(\"Columns renamed and cast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data\n",
    "\n",
    "Let's count the number of incidents for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using Spark SQL\n",
    "df.createOrReplaceTempView(\"crime\")\n",
    "sqlDF = spark.sql(\"\"\"\n",
    "    SELECT Category, COUNT(*) AS Count \n",
    "    FROM crime \n",
    "    GROUP BY Category \n",
    "    ORDER BY Count DESC\n",
    "\"\"\")\n",
    "sqlDF.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using DataFrame functions\n",
    "df.groupBy(\"Category\").count().orderBy(\"count\", ascending=False).show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Using RDD functions\n",
    "rdd = crimes.map(lambda line: (line[1], 1))\n",
    "sorted_counts = sorted(rdd.countByKey().items(), key=lambda x: -x[1])\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the number of incidents at each district?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: SQL\n",
    "sqlDF = spark.sql(\"\"\"\n",
    "    SELECT PdDistrict, COUNT(*) AS Count \n",
    "    FROM crime \n",
    "    GROUP BY PdDistrict \n",
    "    ORDER BY Count DESC\n",
    "\"\"\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: DataFrame\n",
    "df.groupBy(\"PdDistrict\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: RDD functions\n",
    "rdd_district = crimes.map(lambda line: (line[6], 1))\n",
    "sorted_district_counts = sorted(rdd_district.countByKey().items(), key=lambda x: -x[1])\n",
    "print(sorted_district_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define \"downtown\" as an area within 0.005 degrees from (37.792489, -122.403221). \n",
    "Let's count the number of incidents on Sundays within this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: SQL\n",
    "sqlDF = spark.sql(\"\"\"\n",
    "    SELECT Date, DayOfWeek, COUNT(*) AS Count \n",
    "    FROM crime \n",
    "    WHERE DayOfWeek = 'Sunday'\n",
    "      AND POW(Latitude - 37.792489, 2) + POW(Longitude + 122.403221, 2) < POW(0.005, 2)\n",
    "    GROUP BY Date, DayOfWeek \n",
    "    ORDER BY Date\n",
    "\"\"\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: DataFrame\n",
    "df_downtown = df.filter((df['Latitude'] - 37.792489)**2 + (df['Longitude'] + 122.403221)**2 < 0.005**2)\n",
    "df_downtown.filter(df_downtown['DayOfWeek'] == 'Sunday') \\\n",
    "    .groupBy(\"Date\", \"DayOfWeek\") \\\n",
    "    .count() \\\n",
    "    .orderBy('Date') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Spatial Distribution\n",
    "\n",
    "Let's make a scatter plot of the crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the Spark DataFrame to Pandas for plotting\n",
    "pdf = df.select(\"Longitude\", \"Latitude\").toPandas()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('SF Crime Distribution')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.scatter(pdf['Longitude'], pdf['Latitude'], s=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with Spark ML\n",
    "\n",
    "Spark ML requires that features be in a single vector column. We'll use the `VectorAssembler` to\n",
    "combine the `Longitude` and `Latitude` columns into one features column, and then fit a k-means\n",
    "model (with k=3, chosen arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_coor = df.select(\"Longitude\", \"Latitude\")\n",
    "vecAssembler = VectorAssembler(inputCols=[\"Longitude\", \"Latitude\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df_coor).select(\"features\")\n",
    "new_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a k-means model with k=3 and a fixed seed for reproducibility\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(new_df)\n",
    "\n",
    "# Print cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "# Show cluster memberships\n",
    "transformed = model.transform(new_df)\n",
    "transformed.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Clustering Results\n",
    "\n",
    "You can now visualize the clusters (for example, by converting the predictions back to Pandas\n",
    "and plotting them with different colors). Below is one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_clusters = transformed.select(\"features\", \"prediction\").toPandas()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('KMeans Clustering of SF Crimes')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.scatter(pdf_clusters['features'].apply(lambda x: x[0]),\n",
    "            pdf_clusters['features'].apply(lambda x: x[1]),\n",
    "            c=pdf_clusters['prediction'], cmap='viridis', s=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Python 3)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
